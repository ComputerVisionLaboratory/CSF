# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00_dataset.ipynb (unless otherwise specified).

__all__ = ['YoloDataset', 'XMLDetectionDataset', 'CLASSES', 'XMLDetectionDataModule']

# Cell
import collections
import os
import random
import tarfile
import warnings
import xml.etree.ElementTree as ET
import glob
import pandas as pd

import fastai.vision.augment
import fastai.vision.data
import matplotlib.pyplot as plt
import numpy as np
import torch
import torchvision
from cv2 import cv2

from pathlib import Path
from PIL import Image, ImageDraw
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader

from torchvision import transforms as transform_lib
from torchvision.datasets import MNIST, ImageFolder, VOCDetection
from torchvision.datasets.vision import VisionDataset
from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import pytorch_lightning as pl
from pytorch_lightning import LightningDataModule
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.metrics.functional import classification
from rich import print
from rich.progress import track

from ball_detection import utils

# Cell

class YoloDataset(object):
    def __init__(self, root, transforms=None):
        self.root = root
        self.transforms = transforms

        # load all images and annotations, sorting them to
        # ensure that they are aligned
        self.frames = list(sorted(glob.glob(os.path.join(root, "frames/*"))))
        self.annotations = list(sorted(glob.glob(os.path.join(root, "annotations/*"))))
        for f,a in zip(self.frames, self.annotations):
            assert os.path.basename(os.path.splitext(f)[0]) == os.path.basename(os.path.splitext(a)[0]), \
            (os.path.splitext(f)[0], os.path.splitext(a)[0])

        # load label names
        self.label_names = pd.read_csv(os.path.join(root, 'obj.names'), header=None)[0].to_list()
        self.label_colors = [utils.random_color() for _ in self.label_names]

    def __getitem__(self, idx):
        # load images and masks
        img_path = self.frames[idx]
        annot_path = self.annotations[idx]

        img = Image.open(img_path)
        annot = pd.read_csv(annot_path,
                            names=['label', 'x_center', 'y_center', 'width', 'height'],
                            sep=' ')

        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, annot

    def __len__(self):
        return len(self.frames)


    def show_labels(self, idx):
        img, annot = self[idx]
        img_w, img_h = img.size

        img = utils.pil2cv(img)
        for _, obj in annot.iterrows():
            label_idx = int(obj['label'])
            label = self.label_names[label_idx]
            color = self.label_colors[label_idx]

            xc, yc = obj['x_center'] * img_w, obj['y_center'] * img_h
            w, h = obj['width'] * img_w, obj['height'] * img_h

            x1, y1 = xc - w / 2, yc - h / 2
            x2, y2 = xc + w / 2, yc + h / 2
            cv2.rectangle(
              img,
              (int(x1), int(y1)),
              (int(x2), int(y2)),
              color=color,
              thickness=2
            )
            ((label_width, label_height), _) = cv2.getTextSize(
                label,
                fontFace=cv2.FONT_HERSHEY_PLAIN,
                fontScale=1.75,
                thickness=2
            )
            cv2.rectangle(
              img,
              (int(x1), int(y1)),
              (int(x1 + label_width + label_width * 0.05), int(y1 + label_height + label_height * 0.25)),
              color=color,
              thickness=cv2.FILLED
            )
            cv2.putText(
              img,
              label,
              org=(int(x1), int(y1 + label_height + label_height * 0.25)), # bottom left
              fontFace=cv2.FONT_HERSHEY_PLAIN,
              fontScale=1.75,
              color=(255, 255, 255),
              thickness=2
            )
        return utils.cv2pil(img)

# Cell

CLASSES = ["soccer_ball"]

class XMLDetectionDataset(VisionDataset):
    """`Pascal VOC <http://host.robots.ox.ac.uk/pascal/VOC/>`_ Detection Dataset.

    Args:
        root (string): Root directory of the VOC Dataset.
        image_transform (callable, optional): A function/transform that  takes in an PIL image
            and returns a transformed version. E.g, ``transforms.RandomCrop``
        target_transform (callable, required): A function/transform that takes in the
            target and transforms it.
        transforms (callable, optional): A function/transform that takes input sample and its target as entry
            and returns a transformed version.
    """

    def __init__(
        self,
        root: str,
        image_transform: Optional[Callable] = None,
        target_transform: Optional[Callable] = None,
        transform: Optional[Callable] = None,
    ):
        super(XMLDetectionDataset, self).__init__(root)
        self.root = Path(root)
        self.image_transform = image_transform
        self.target_transform = target_transform
        self.transform = transform
        self.add_background_bbox = False
        self.image_files = sorted(list(self.root.glob("*.jpg")))
        self.xml_files = sorted(list(self.root.glob("*.xml")))

        assert len(self.image_files) == len(self.xml_files),\
            f"{len(self.image_files), len(self.xml_files)}"

    def __getitem__(self, index: int) -> Tuple[Any, Any]:
        """dataset[0]のときにdataset.__getitem__(0)が実行される
        Args:
            index (int): Index

        Returns:
            tuple: (image, target) where target is a dictionary of the XML tree.
        """
        img = np.asarray(Image.open(str(self.image_files[index])).convert("RGB"))
        target = self.parse_xml(ET.parse(str(self.xml_files[index])).getroot())
        bbox = self.parse_bboxes(target)
        class_labels = self.parse_labels(target)

        if self.add_background_bbox:
            bbox.append((0, 0, *img.T.shape[-2:]))
            class_labels.append('background')
        else:
            if len(bbox)==0:
                new_idx = random.choice(range(self.__len__()))
                return self.__getitem__(new_idx)

        if self.transform is not None:
            target = self.transform(image=img, bboxes=bbox, class_labels=class_labels)
            img = target['image']

        if self.image_transform:
            img = self.image_transform(img)
        if self.target_transform:
            target = self.target_transform(target)

        return img, target

    def __len__(self) -> int:
        return len(self.image_files)

    def parse_xml(self, node: ET.Element) -> Dict[str, Any]:
        xml_dict: Dict[str, Any] = {}
        children = list(node)
        if children:
            def_dic: Dict[str, Any] = collections.defaultdict(list)
            for dc in map(self.parse_xml, children):
                for ind, v in dc.items():
                    def_dic[ind].append(v)
            if node.tag == "annotation":
                def_dic["object"] = [def_dic["object"]]
            xml_dict = {
                node.tag: {
                    ind: v[0] if len(v) == 1 else v for ind, v in def_dic.items()
                }
            }
        if node.text:
            text = node.text.strip()
            if not children:
                xml_dict[node.tag] = text
        return xml_dict

    def parse_bboxes(self, xml):
        _bboxes = xml['annotation']['object']
#         assert len(bboxes) >= 1, f'File contains no/more than one instance of ball: {xml}'
        bboxes = []
        for bbox in _bboxes:
            bbox = bbox['bndbox']
            xmin = int(float(bbox['xmin']))
            ymin = int(float(bbox['ymin']))
            xmax = int(float(bbox['xmax']))
            ymax = int(float(bbox['ymax']))
            bboxes.append((xmin, ymin,xmax,ymax))
        return bboxes

    def parse_labels(self, xml):
        return [x['name'] for x in xml['annotation']['object']]

    def draw_sample(self, idx=None):
        if idx is None:
            idx = random.choice(range(0, len(self)))

        img = Image.open(str(self.image_files[idx])).convert("RGB")
        target = self.parse_xml(ET.parse(str(self.xml_files[idx])).getroot())
        x0, y0, x1, y1 = self.parse_bboxes(target)[0]

        draw = ImageDraw.Draw(img)
        draw.rectangle([x0, y0, x1, y1])

        return img
        plt.imshow(draw)
        plt.show()

# Cell

class XMLDetectionDataModule(LightningDataModule):
    def __init__(
        self,
        data_dir: str,
        r_train: float =None,
        r_val: float = None,
        r_test: float = None,
        batch_size: int = 16,
        num_workers: int = 16,
        normalize: bool = False,
        shuffle: bool = False,
        pin_memory: bool = False,
        drop_last: bool = False,
        transform=None,
        image_transform=None,
        target_transform=None,
        *args: Any,
        **kwargs: Any,
    ) -> None:

        super().__init__(*args, **kwargs)

        self.data_dir = data_dir
        self.r_train = r_train
        self.r_val = r_val
        self.r_test = r_test
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.normalize = normalize
        self.shuffle = shuffle
        self.pin_memory = pin_memory
        self.drop_last = drop_last
        self.transform = transform
        self.image_transform = image_transform
        self.target_transform = target_transform

    def setup(self, mode="fit") -> None:
        if mode == 'use_dir':
            self.trainset = XMLDetectionDataset(
                os.path.join(self.data_dir, 'train'),
                transform=self.transform,
                image_transform=self.image_transform,
                target_transform=self.target_transform)
            self.valset = XMLDetectionDataset(
                os.path.join(self.data_dir, 'valid'),
                transform=self.transform,
                image_transform=self.image_transform,
                target_transform=self.target_transform)
            self.testset = XMLDetectionDataset(
                os.path.join(self.data_dir, 'test'),
                transform=self.transform,
                image_transform=self.image_transform,
                target_transform=self.target_transform)
        else:
            dataset = XMLDetectionDataset(self.data_dir,
                                          transform=self.transform,
                                          image_transform=self.image_transform,
                                          target_transform=self.target_transform)

            n_train = int(len(dataset) * self.r_train)
            n_val = int((len(dataset) - n_train) * self.r_val/ (self.r_test + self.r_val))
            n_test = len(dataset) - n_train - n_val

            self.trainset, self.valset, self.testset = torch.utils.data.random_split(
                dataset, [n_train, n_val, n_test],
            )

    def train_dataloader(
        self,
        batch_size: int = 32,
        image_transforms: Union[List[Callable], Callable] = None,
    ) -> DataLoader:
        """
        VOCDetection train set uses the `train` subset
        Args:
            batch_size: size of batch
            transforms: custom transforms
        """
        return DataLoader(
            self.trainset,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn,
        )

    def val_dataloader(
        self, batch_size: int = 32, image_transforms: Optional[List[Callable]] = None
    ) -> DataLoader:
        """
        VOCDetection val set uses the `val` subset
        Args:
            batch_size: size of batch
            transforms: custom transforms
        """
        return DataLoader(
            self.valset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn,
        )

    def test_dataloader(
        self, batch_size: int = 1, image_transforms: Optional[List[Callable]] = None
    ) -> DataLoader:
        """
        VOCDetection val set uses the `val` subset
        Args:
            batch_size: size of batch
            transforms: custom transforms
        """
        return DataLoader(
            self.testset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            drop_last=self.drop_last,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn,
        )

    def _collate_fn(self, batch):
        return tuple(zip(*batch))